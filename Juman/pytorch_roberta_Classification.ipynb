{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ＊PYTORCHによるRoBERTaの実装\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 環境:Ubuntu20.04\n",
    "- GPU:NVIDIA RTX A6000\n",
    "- ドライバー:NVIDIA-SMI 510.47.03, Driver Version: 510.47.03, CUDA Version: 11.6\n",
    "- ./data:train.tsv, test.tsv,test.csv\n",
    "- EarlyStoppingを利用する場合はhttps://github.com/Bjarten/early-stopping-pytorch からpytorchtools.pyをutilsにインストールし学習・検証のコメントアウトを外すこと"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ＊事前準備\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#https://pytorch.org/get-started/previous-versions/\n",
    "#https://github.com/pytorch/text/issues/1342\n",
    "#https://github.com/pytorch/text#installation\n",
    "#https://teratail.com/questions/358588\n",
    "\n",
    "!conda create -n pytorch\n",
    "!conda activate pytorch\n",
    "# https://pytorch.org/get-started/previous-versions/#linux-and-windows-11\n",
    "!pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install jupyterlab\n",
    "\n",
    "!pip install transformers==4.17.0\n",
    "!pip install torchtext==0.9.0\n",
    "!pip install tqdm\n",
    "!pip install pyknp\n",
    "!pip install attrdict\n",
    "!pip install spacy\n",
    "!pip install mojimoji\n",
    "!pip install protobuf\n",
    "!pip install sentencepiece\n",
    "!pip install fugashi\n",
    "!pip install pandas\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "#print(torch.cuda.device(0))                                                    \n",
    "print(torch.version.cuda)\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "#True\n",
    "#11.1\n",
    "#1.8.0+cu111\n",
    "#0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ＊ライブラリのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch \n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "import glob\n",
    "import os\n",
    "import io\n",
    "import string\n",
    "import re\n",
    "import sys\n",
    "import random\n",
    "import mojimoji\n",
    "import string\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertJapaneseTokenizer, BertModel, AutoTokenizer\n",
    "path_result=\"./result/\"\n",
    "path_weights=\"./weights/\"\n",
    "max_length=128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ＊Jumanで形態素解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyknp import Juman\n",
    "class JumanTokenize(object):\n",
    "    def __init__(self):\n",
    "        self.juman = Juman()\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        result = self.juman.analysis(text)\n",
    "        return [mrph.midasi for mrph in result.mrph_list()]\n",
    "    \n",
    "class BertTokenizer(object):\n",
    "    def __init__(self):  \n",
    "        self.juman_tokenizer = JumanTokenize()\n",
    "    def tokenize(self, text):\n",
    "        split_tokens = []\n",
    "        token = self.juman_tokenizer.tokenize(text)\n",
    "        \"\"\"\n",
    "        for token in self.juman_tokenizer.tokenize(text):\n",
    "                print(token)\n",
    "                split_tokens.append(token)\n",
    "        \"\"\"\n",
    "        #print(token)\n",
    "        return token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ＊前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_text(text):\n",
    "    # 半角・全角の統一（半角から全角へ変換）\n",
    "    text = mojimoji.han_to_zen(text) \n",
    "    # 改行、半角スペース、全角スペースを削除\n",
    "    text = re.sub('\\r', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('　', '', text)\n",
    "    text = re.sub(' ', '', text)\n",
    "    #どっちでも\n",
    "    #text = re.sub(',', '', text)\n",
    "\n",
    "    # 数字文字の一律「0」化\n",
    "    #text = re.sub(r'[0-9 ０-９]+', '0', text)  # 数字\n",
    "\n",
    "    # カンマ、ピリオド以外の記号をスペースに置換\n",
    "    for p in string.punctuation:\n",
    "        #if (p == \".\"):\n",
    "        if (p == \".\") or (p == \",\"):\n",
    "            continue\n",
    "        else:\n",
    "            text = text.replace(p, \" \")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ＊TorchtextでDatasetの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乱数のシードを設定\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)\n",
    "\n",
    "# 単語分割用のTokenizerを用意\n",
    "tokenizer_bert=AutoTokenizer.from_pretrained(\"nlp-waseda/roberta-base-japanese\")\n",
    "#tokenizer_bert = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-v2\")\n",
    "#tokenizer_bert = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")\n",
    "\n",
    "tokenizer_bert_Juman = BertTokenizer()\n",
    "\n",
    "def tokenizer_with_preprocessing(text, tokenizer=tokenizer_bert_Juman):\n",
    "    text = preprocessing_text(text)\n",
    "    token = tokenizer_bert_Juman.tokenize(text)\n",
    "    ret = tokenizer_bert.encode(\" \".join(token), max_length=max_length, truncation=True, return_tensors='pt')[0]\n",
    "    #print(ret)\n",
    "    return ret\n",
    "\n",
    "# データを読み込んだときに、読み込んだ内容に対して行う処理を定義します\n",
    "TEXT = torchtext.legacy.data.Field(sequential=True, tokenize=tokenizer_with_preprocessing, use_vocab=False, lower=False, include_lengths=True, batch_first=True, fix_length=max_length, pad_token=0)\n",
    "\n",
    "LABEL = torchtext.legacy.data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "# フォルダ「data」から各tsvファイルを読み込みます\n",
    "train_val_ds, test_ds = torchtext.legacy.data.TabularDataset.splits(\n",
    "    path=\"data\", train='train.tsv',\n",
    "    test='test.tsv', format='tsv',\n",
    "    fields=[('Text', TEXT), ('Label', LABEL)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ＊Dataloaderの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTでは16、32あたりを使用する\n",
    "batch_size=32\n",
    "\n",
    "# torchtext.data.Datasetのsplit関数で訓練データとvalidationデータを分ける\n",
    "train_ds, val_ds = train_val_ds.split(split_ratio=0.8, random_state=random.seed(1234))\n",
    "\n",
    "train_dl = torchtext.legacy.data.Iterator(train_val_ds, batch_size=batch_size, train=True)\n",
    "train_dl_val = torchtext.legacy.data.Iterator(train_ds, batch_size=batch_size, train=True)\n",
    "val_dl = torchtext.legacy.data.Iterator(val_ds, batch_size=batch_size, train=False, sort=False)\n",
    "test_dl = torchtext.legacy.data.Iterator(test_ds, batch_size=batch_size, train=False, sort=False)\n",
    "\n",
    "# 辞書オブジェクトにまとめる\n",
    "dataloaders_dict_val = {\"train\": train_dl_val, \"val\": val_dl}\n",
    "dataloaders_dict = {\"train\": train_dl}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ＊動作確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 動作確認 テストデータのデータセットで確認\n",
    "batch = next(iter(test_dl))\n",
    "print(batch)\n",
    "print(batch.Text)\n",
    "print(batch.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ミニバッチの1文目を確認してみる\n",
    "text_minibatch_1 = (batch.Text[0][1]).numpy()\n",
    "\n",
    "# IDを単語に戻す\n",
    "text = tokenizer_bert.convert_ids_to_tokens(text_minibatch_1)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 動作確認\n",
    "tokenizer_bert_Juman = BertTokenizer()\n",
    "text=\"早稲田大学で自然言語処理の勉強をする\"\n",
    "text = tokenizer_bert_Juman.tokenize(text)\n",
    "tokenizer_bert=AutoTokenizer.from_pretrained(\"nlp-waseda/roberta-base-japanese\")\n",
    "text=tokenizer_bert(\" \".join(text))\n",
    "print(text)\n",
    "\n",
    "# IDを単語に戻す\n",
    "text2=tokenizer_bert.convert_ids_to_tokens(text[\"input_ids\"])\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ＊RoBERTaモデルを構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/huggingface/transformers/issues/5421\n",
    "from transformers import BertModel, RobertaModel, AutoModelForMaskedLM\n",
    "\n",
    "net_bert=RobertaModel.from_pretrained(\"nlp-waseda/roberta-base-japanese\")\n",
    "#net_bert=BertModel.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")\n",
    "print(net_bert.config)\n",
    "\n",
    "#model= AutoModelForMaskedLM.from_pretrained(\"nlp-waseda/roberta-base-japanese\")\n",
    "#net_bert = RobertaModel.from_pretrained(model.config, add_pooling_layer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py\n",
    "#https://github.com/huggingface/transformers/issues/8776\n",
    "#https://github.com/huggingface/transformers/issues/1328\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, net_bert):\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        # BERTモジュール\n",
    "        self.bert = net_bert  # BERTモデル   \n",
    "        self.classifier = RobertaClassificationHead(net_bert.config)\n",
    "\n",
    "        # headに予測を追加\n",
    "        # 入力はBERTの出力特徴量の次元、出力は2つ\n",
    "        self.cls = nn.Linear(in_features=768, out_features=2)\n",
    "        #増やす場合はout_reaturesを変更する\n",
    "        #self.cls = nn.Linear(in_features=768, out_features=3)\n",
    "        \n",
    "        #　ドロップアウト率\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # 重み初期化処理\n",
    "        nn.init.normal_(self.cls.weight, std=0.02)\n",
    "        nn.init.normal_(self.cls.bias, 0)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=False, attention_show_flg=False):\n",
    "        '''\n",
    "        input_ids： [batch_size, sequence_length]の文章の単語IDの羅列\n",
    "        token_type_ids： [batch_size, sequence_length]の、各単語が1文目なのか、2文目なのかを示すid\n",
    "        attention_mask：Transformerのマスクと同じ働きのマスキングです\n",
    "        output_all_encoded_layers：最終出力に12段のTransformerの全部をリストで返すか、最後だけかを指定\n",
    "        attention_show_flg：Self-Attentionの重みを返すかのフラグ\n",
    "        '''\n",
    "\n",
    "        # BERTの基本モデル部分の順伝搬\n",
    "        # 順伝搬させる\n",
    "        if attention_show_flg == True:\n",
    "            output  = self.bert(input_ids, output_attentions=True, output_hidden_states=True)\n",
    "            encoded_layers = output['last_hidden_state']\n",
    "            attentions = output['attentions']\n",
    "            pooler_output = output['pooler_output'] \n",
    "            \n",
    "        elif attention_show_flg == False:\n",
    "            output  = self.bert(input_ids)\n",
    "            #output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers, attention_show_flg)\n",
    "            encoded_layers = output['last_hidden_state']\n",
    "            pooler_output = output['pooler_output'] \n",
    "        \n",
    "        # 書籍の実装：入力文章の1単語目[CLS]の特徴量を使用して、分類します\n",
    "        #vec_0 = encoded_layers[:, 0, :]\n",
    "        #vec_0 = vec_0.view(-1, 768)  # sizeを[batch_size, hidden_sizeに変換\n",
    "        #out = self.cls(vec_0)\n",
    "        \n",
    "        # Hugging faceの実装：RobertaPoolerとの違いはdropoutのみ?\n",
    "        #sequence_output = output['last_hidden_state']\n",
    "        #out = self.classifier(sequence_output)\n",
    "        \n",
    "        # オリジナル：Hugging faceの実装とほとんど同じ（違いはdropoutのみ?）\n",
    "        pooler_output = self.dropout(pooler_output)\n",
    "        out = self.cls(pooler_output)\n",
    "\n",
    "        # attention_showのときは、attention_probs（1番最後の）もリターンする\n",
    "        if attention_show_flg == True:\n",
    "            return out, attentions\n",
    "        elif attention_show_flg == False:\n",
    "            return out\n",
    "        \n",
    "class RobertaClassificationHead(nn.Module):\n",
    "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        classifier_dropout = (\n",
    "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
    "        )\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル構築\n",
    "net = BertClassifier(net_bert)\n",
    "# 訓練モードに設定\n",
    "net.train()\n",
    "\n",
    "print('ネットワーク設定完了')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ＊BERTのファインチューニング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パターン1：全てのパラメータを更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTよりも学習率を大きくしないと学習しない\n",
    "# 勾配計算True\n",
    "for name, param in net.named_parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "# 最適化手法の設定\n",
    "# BERTの元の部分はファインチューニング\n",
    "optimizer = optim.Adam([\n",
    "    {'params': net.cls.parameters(), 'lr': 5e-3}\n",
    "], betas=(0.9, 0.999))\n",
    "\n",
    "# 損失関数の設定\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# nn.LogSoftmax()を計算してからnn.NLLLoss(negative log likelihood loss)を計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パターン2：BERTの最終層と識別器のパラメータを更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 勾配計算を最後のBertLayerモジュールと追加した分類アダプターのみ実行\n",
    "\n",
    "# 1. まず全部を、勾配計算Falseにしてしまう\n",
    "for name, param in net.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 2. 最後のBertLayerモジュールを勾配計算ありに変更\n",
    "for name, param in net.bert.encoder.layer[-1].named_parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 3. 識別器を勾配計算ありに変更\n",
    "for name, param in net.cls.named_parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "# 最適化手法の設定\n",
    "\n",
    "# BERTの元の部分はファインチューニング\n",
    "optimizer = optim.Adam([\n",
    "    {'params': net.bert.encoder.layer[-1].parameters(), 'lr': 5e-5},\n",
    "    {'params': net.cls.parameters(), 'lr': 1e-5}\n",
    "], betas=(0.9, 0.999))\n",
    "\n",
    "# 損失関数の設定\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# nn.LogSoftmax()を計算してからnn.NLLLoss(negative log likelihood loss)を計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## パターン3：BERTの最終4層と識別器のパラメータを更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# まず全部を、勾配計算Falseにしてしまう\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# BERTの最終4層分を勾配計算ありに変更\n",
    "for param in net.bert.encoder.layer[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in net.bert.encoder.layer[-2].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in net.bert.encoder.layer[-3].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in net.bert.encoder.layer[-4].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 識別器を勾配計算ありに変更\n",
    "for param in net.cls.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 事前学習済の箇所は学習率小さめ、最後の全結合層は大きめにする\n",
    "optimizer = optim.Adam([\n",
    "    {'params': net.bert.encoder.layer[-1].parameters(), 'lr': 5e-5},\n",
    "    {'params': net.bert.encoder.layer[-2].parameters(), 'lr': 5e-5},\n",
    "    {'params': net.bert.encoder.layer[-3].parameters(), 'lr': 5e-5},\n",
    "    {'params': net.bert.encoder.layer[-4].parameters(), 'lr': 5e-5},\n",
    "    {'params': net.cls.parameters(), 'lr': 1e-4}\n",
    "])\n",
    "\n",
    "# 損失関数の設定\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# nn.LogSoftmax()を計算してからnn.NLLLoss(negative log likelihood loss)を計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ＊学習・検証"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ＊ 開発データでハイパーパラメータを決定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを学習させる関数を作成\n",
    "#from utils.pytorchtools import EarlyStopping\n",
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "    #エポック数,Acuraccy,Loss保存用\n",
    "    Epochs=[]\n",
    "    Accuracy_train=[]\n",
    "    Loss_train=[]\n",
    "    Accuracy_val=[]\n",
    "    Loss_val=[]\n",
    "    \n",
    "    #イテレータのエポック数,Acuraccy,Loss保存用\n",
    "    Epochs_it=[]\n",
    "    Accuracy_train_it=[]\n",
    "    Loss_train_it=[]\n",
    "    \n",
    "    # GPUが使えるかを確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #device = torch.device(\"cpu\")\n",
    "    print(\"使用デバイス：\", device)\n",
    "    print('-----start-------')\n",
    "    #early_stopping = EarlyStopping(patience=10)\n",
    "    \n",
    "    # ネットワークをGPUへ\n",
    "    net.to(device)\n",
    "\n",
    "    # ネットワークがある程度固定であれば、高速化させる\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # ミニバッチのサイズ\n",
    "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
    "    n=0\n",
    "    \n",
    "    #時間\n",
    "    start =time.time()\n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs):\n",
    "        \"\"\"\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early Stopping\")\n",
    "            break # 打ち切り\n",
    "        # epochごとの訓練と検証のループ\n",
    "        \"\"\"\n",
    "        \n",
    "        for phase in ['train', 'val']:\n",
    "            count=0\n",
    "            if phase == 'train':\n",
    "                net.train()  # モデルを訓練モードに\n",
    "            else:\n",
    "                net.eval()   # モデルを検証モードに\n",
    "                count=1\n",
    "            \n",
    "            epoch_loss = 0.0  # epochの損失和\n",
    "            epoch_corrects = 0  # epochの正解数\n",
    "            iteration = 1\n",
    "\n",
    "            # 開始時刻を保存\n",
    "            t_epoch_start = time.time()\n",
    "            t_iter_start = time.time()\n",
    "\n",
    "            # データローダーからミニバッチを取り出すループ\n",
    "            #print(dataloaders_dict[phase])\n",
    "            for batch in (dataloaders_dict[phase]):\n",
    "                # batchはTextとLableの辞書型変数\n",
    "\n",
    "                # GPUが使えるならGPUにデータを送る\n",
    "                inputs = batch.Text[0].to(device)  # 文章\n",
    "                labels = batch.Label.to(device)  # ラベル\n",
    "\n",
    "                # optimizerを初期化\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 順伝搬（forward）計算\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "\n",
    "                    # Bertに入力\n",
    "                    outputs = net(inputs, token_type_ids=None, attention_mask=None,\n",
    "                                  output_all_encoded_layers=False, attention_show_flg=False)\n",
    "\n",
    "                    loss = criterion(outputs, labels)  # 損失を計算\n",
    "                    #loss = criterion(outputs.view(-1, 2), labels.view(-1))\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
    "                    \n",
    "\n",
    "                    # 訓練時はバックプロパゲーション\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        \n",
    "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            acc = (torch.sum(preds == labels.data)).double()/batch_size\n",
    "                            print('イテレーション {} || Loss: {:.4f} || 10iter: {:.4f} sec. || 本イテレーションの正解率：{}'.format(iteration, loss.item(), duration, acc))\n",
    "                            t_iter_start = time.time()\n",
    "                            \n",
    "                            \"\"\"\n",
    "                            Epochs_it.append(iteration/10)\n",
    "                            Accuracy_train_it.append(acc)\n",
    "                            Loss_train_it.append(loss.item())\n",
    "                            \"\"\"\n",
    "                            #early_stopping(loss.item(), net)\n",
    "                                       \n",
    "\n",
    "                    iteration += 1\n",
    "\n",
    "                    # 損失と正解数の合計を更新\n",
    "                    epoch_loss += loss.item() * batch_size\n",
    "                    epoch_corrects += torch.sum(preds == labels.data)\n",
    "                    \n",
    "            # epochごとのlossと正解率\n",
    "            t_epoch_finish = time.time()\n",
    "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "            epoch_acc = epoch_corrects.double(\n",
    "            ) / len(dataloaders_dict[phase].dataset)\n",
    "\n",
    "            print('Epoch {}/{} | {:^5} |  Loss: {:.4f} Acc: {:.4f}'.format(epoch+1, num_epochs,\n",
    "                                                                           phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            a=float(\"{:.4f}\".format(epoch_acc))\n",
    "                \n",
    "            t_epoch_start = time.time()\n",
    "            \n",
    "            if count == 0:\n",
    "                Loss_train.append(epoch_loss)\n",
    "                Accuracy_train.append(a)\n",
    "            elif count==1:\n",
    "                Loss_val.append(epoch_loss)\n",
    "                Accuracy_val.append(a)\n",
    "        Epochs.append(epoch+1)\n",
    "    t=time.time()\n",
    "    print(\"Time:{:.4f}sec\".format(t-start))\n",
    "        \n",
    "    return net,Epochs,Loss_train,Accuracy_train,Loss_val,Accuracy_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 学習・検証を実行する。1epochに20分ほどかかります\n",
    "num_epochs = 10\n",
    "net_trained, Epochs, Loss_train, Accuracy_train, Loss_val, Accuracy_val = train_model(net, dataloaders_dict_val,\n",
    "                          criterion, optimizer, num_epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ＊学習時のEpochsごとのAccuracyを出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#print(Accuracy_train)\n",
    "#print(Accuracy_val)\n",
    "#Accuracy_test=[]\n",
    "plt.plot(Epochs,Accuracy_train,color=\"blue\",label=\"train\")\n",
    "plt.plot(Epochs,Accuracy_val,color=\"red\",label=\"val\")\n",
    "#plt.plot(Epochs,Accuracy_test,color=\"red\",label=\"test\")\n",
    "plt.xticks(Epochs)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig(path_result+\"acc_val.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ＊学習時のEpochsごとのLossを出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.axes().set_aspect(\"equal\")\n",
    "#Loss_test=[]\n",
    "plt.plot(Epochs, Loss_train,color=\"blue\",label=\"train\")\n",
    "plt.plot(Epochs, Loss_val,color=\"red\",label=\"val\")\n",
    "#plt.plot(Epochs,Loss_test,color=\"red\",label=\"test\")\n",
    "plt.xticks(Epochs)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig(path_result+\"Loss_val.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ＊ 全データで学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを学習させる関数を作成\n",
    "#from utils.pytorchtools import EarlyStopping\n",
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "    \n",
    "    #エポック数,Acuraccy,Loss保存用\n",
    "    Epochs=[]\n",
    "    Accuracy_train=[]\n",
    "    Loss_train=[]\n",
    "    Accuracy_val=[]\n",
    "    Loss_val=[]\n",
    "    \n",
    "    #イテレータのエポック数,Acuraccy,Loss保存用\n",
    "    Epochs_it=[]\n",
    "    Accuracy_train_it=[]\n",
    "    Loss_train_it=[]\n",
    "    \n",
    "    # GPUが使えるかを確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #device = torch.device(\"cpu\")\n",
    "    print(\"使用デバイス：\", device)\n",
    "    print('-----start-------')\n",
    "    #early_stopping = EarlyStopping(patience=10)\n",
    "    \n",
    "    # ネットワークをGPUへ\n",
    "    net.to(device)\n",
    "\n",
    "    # ネットワークがある程度固定であれば、高速化させる\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # ミニバッチのサイズ\n",
    "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
    "    n=0\n",
    "    \n",
    "    #時間\n",
    "    start =time.time()\n",
    "    \n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs):\n",
    "        \"\"\"\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early Stopping\")\n",
    "            break # 打ち切り\n",
    "        # epochごとの訓練と検証のループ\n",
    "        \"\"\"\n",
    "        \n",
    "        #for phase in ['train', 'val']:\n",
    "        for phase in ['train']:\n",
    "            count=0\n",
    "            if phase == 'train':\n",
    "                net.train()  # モデルを訓練モードに\n",
    "            else:\n",
    "                net.eval()   # モデルを検証モードに\n",
    "                count=1\n",
    "            \n",
    "            epoch_loss = 0.0  # epochの損失和\n",
    "            epoch_corrects = 0  # epochの正解数\n",
    "            iteration = 1\n",
    "\n",
    "            # 開始時刻を保存\n",
    "            t_epoch_start = time.time()\n",
    "            t_iter_start = time.time()\n",
    "\n",
    "            # データローダーからミニバッチを取り出すループ\n",
    "            for batch in (dataloaders_dict[phase]):\n",
    "                # batchはTextとLableの辞書型変数\n",
    "\n",
    "                # GPUが使えるならGPUにデータを送る\n",
    "                inputs = batch.Text[0].to(device)  # 文章\n",
    "                labels = batch.Label.to(device)  # ラベル\n",
    "\n",
    "                # optimizerを初期化\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 順伝搬（forward）計算\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "\n",
    "                    # BertForIMDbに入力\n",
    "                    outputs = net(inputs, token_type_ids=None, attention_mask=None,\n",
    "                                  output_all_encoded_layers=False, attention_show_flg=False)\n",
    "\n",
    "                    loss = criterion(outputs, labels)  # 損失を計算\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
    "                    \n",
    "\n",
    "                    # 訓練時はバックプロパゲーション\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        \n",
    "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            acc = (torch.sum(preds == labels.data)).double()/batch_size\n",
    "                            print('イテレーション {} || Loss: {:.4f} || 10iter: {:.4f} sec. || 本イテレーションの正解率：{}'.format(iteration, loss.item(), duration, acc))\n",
    "                            t_iter_start = time.time()\n",
    "                            \n",
    "                            \"\"\"\n",
    "                            Epochs_it.append(iteration/10)\n",
    "                            Accuracy_train_it.append(acc)\n",
    "                            Loss_train_it.append(loss.item())\n",
    "                            \"\"\"\n",
    "                            #early_stopping(loss.item(), net)\n",
    "                                       \n",
    "\n",
    "                    iteration += 1\n",
    "\n",
    "                    # 損失と正解数の合計を更新\n",
    "                    epoch_loss += loss.item() * batch_size\n",
    "                    epoch_corrects += torch.sum(preds == labels.data)\n",
    "                    \n",
    "            # epochごとのlossと正解率\n",
    "            t_epoch_finish = time.time()\n",
    "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "            epoch_acc = epoch_corrects.double(\n",
    "            ) / len(dataloaders_dict[phase].dataset)\n",
    "\n",
    "            print('Epoch {}/{} | {:^5} |  Loss: {:.4f} Acc: {:.4f}'.format(epoch+1, num_epochs,\n",
    "                                                                           phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            a=float(\"{:.4f}\".format(epoch_acc))\n",
    "                \n",
    "            t_epoch_start = time.time()\n",
    "            \n",
    "            if count == 0:\n",
    "                Loss_train.append(epoch_loss)\n",
    "                Accuracy_train.append(a)\n",
    "            elif count==1:\n",
    "                Loss_val.append(epoch_loss)\n",
    "                Accuracy_val.append(a)\n",
    "        Epochs.append(epoch+1)\n",
    "    t=time.time()\n",
    "    time_val = t-start\n",
    "    print(\"Time:{:.4f}sec\".format(time_val ))\n",
    "        \n",
    "    return net, Epochs, Loss_train, Accuracy_train, Loss_val, Accuracy_val, time_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 学習・検証を実行する。1epochに20分ほどかかります\n",
    "num_epochs = 10\n",
    "net_trained, Epochs, Loss_train, Accuracy_train, Loss_val, Accuracy_val, time_val= train_model(net, dataloaders_dict,\n",
    "                          criterion, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習したネットワークパラメータを保存します\n",
    "save_path = path_weights+'bert_fine_tuning.pth'\n",
    "torch.save(net_trained.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ＊＊＊保存しているモデルを使用する場合＊＊＊\n",
    "- RoBERTaモデルを構築\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, net_bert):\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        # BERTモジュール\n",
    "        self.bert = net_bert  # BERTモデル   \n",
    "        self.classifier = RobertaClassificationHead(net_bert.config)\n",
    "\n",
    "        # headに予測を追加\n",
    "        # 入力はBERTの出力特徴量の次元、出力は2つ\n",
    "        self.cls = nn.Linear(in_features=768, out_features=2)\n",
    "        #増やす場合はout_reaturesを変更する\n",
    "        #self.cls = nn.Linear(in_features=768, out_features=3)\n",
    "        \n",
    "        #　ドロップアウト率\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # 重み初期化処理\n",
    "        nn.init.normal_(self.cls.weight, std=0.02)\n",
    "        nn.init.normal_(self.cls.bias, 0)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=False, attention_show_flg=False):\n",
    "        '''\n",
    "        input_ids： [batch_size, sequence_length]の文章の単語IDの羅列\n",
    "        token_type_ids： [batch_size, sequence_length]の、各単語が1文目なのか、2文目なのかを示すid\n",
    "        attention_mask：Transformerのマスクと同じ働きのマスキングです\n",
    "        output_all_encoded_layers：最終出力に12段のTransformerの全部をリストで返すか、最後だけかを指定\n",
    "        attention_show_flg：Self-Attentionの重みを返すかのフラグ\n",
    "        '''\n",
    "\n",
    "        # BERTの基本モデル部分の順伝搬\n",
    "        # 順伝搬させる\n",
    "        if attention_show_flg == True:\n",
    "            output  = self.bert(input_ids, output_attentions=True, output_hidden_states=True)\n",
    "            encoded_layers = output['last_hidden_state']\n",
    "            attentions = output['attentions']\n",
    "            pooler_output = output['pooler_output'] \n",
    "            \n",
    "        elif attention_show_flg == False:\n",
    "            output  = self.bert(input_ids)\n",
    "            #output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers, attention_show_flg)\n",
    "            encoded_layers = output['last_hidden_state']\n",
    "            pooler_output = output['pooler_output'] \n",
    "        \n",
    "        # 書籍の実装：入力文章の1単語目[CLS]の特徴量を使用して、分類します\n",
    "        #vec_0 = encoded_layers[:, 0, :]\n",
    "        #vec_0 = vec_0.view(-1, 768)  # sizeを[batch_size, hidden_sizeに変換\n",
    "        #out = self.cls(vec_0)\n",
    "        \n",
    "        # Hugging faceの実装：RobertaPoolerとの違いはdropoutのみ?\n",
    "        #sequence_output = output['last_hidden_state']\n",
    "        #out = self.classifier(sequence_output)\n",
    "        \n",
    "        # オリジナル：Hugging faceの実装とほとんど同じ（違いはdropoutのみ?）\n",
    "        pooler_output = self.dropout(pooler_output)\n",
    "        out = self.cls(pooler_output)\n",
    "\n",
    "        # attention_showのときは、attention_probs（1番最後の）もリターンする\n",
    "        if attention_show_flg == True:\n",
    "            return out, attentions\n",
    "        elif attention_show_flg == False:\n",
    "            return out\n",
    "        \n",
    "class RobertaClassificationHead(nn.Module):\n",
    "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        classifier_dropout = (\n",
    "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
    "        )\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル構築\n",
    "net_trained = BertClassifier(net_bert)\n",
    "\n",
    "# 訓練モードに設定\n",
    "net_trained.train()\n",
    "\n",
    "print('ネットワーク設定完了')\n",
    "\n",
    "#学習済みモデルを読み込む\n",
    "model_path = './weights/bert_fine_tuning.pth'\n",
    "net_trained.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "\n",
    "# 損失関数の設定\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# nn.LogSoftmax()を計算してからnn.NLLLoss(negative log likelihood loss)を計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ＊＊＊Juman前処理用（テストデータのみ）＊＊＊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertJapaneseTokenizer, BertModel, AutoTokenizer\n",
    "path_result=\"./result/\"\n",
    "path_weights=\"./weights/\"\n",
    "max_length=128\n",
    "batch_size=16\n",
    "\n",
    "#テストデータのみ\n",
    "# 乱数のシードを設定\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)\n",
    "\n",
    "# 単語分割用のTokenizerを用意\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(\"nlp-waseda/roberta-base-japanese\")\n",
    "#tokenizer_bert = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")\n",
    "\n",
    "def preprocessing_text(text):\n",
    "    # 半角・全角の統一\n",
    "    text = mojimoji.han_to_zen(text) \n",
    "    # 改行、半角スペース、全角スペースを削除\n",
    "    text = re.sub('\\r', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('　', '', text)\n",
    "    text = re.sub(' ', '', text)\n",
    "\n",
    "    # 数字文字の一律「0」化\n",
    "    text = re.sub(r'[0-9 ０-９]+', '0', text)  # 数字\n",
    "\n",
    "    # カンマ、ピリオド以外の記号をスペースに置換\n",
    "    for p in string.punctuation:\n",
    "        #if (p == \".\"):\n",
    "        if (p == \".\") or (p == \",\"):\n",
    "            continue\n",
    "        else:\n",
    "            text = text.replace(p, \" \")\n",
    "        return text\n",
    "\n",
    "# 前処理と単語分割をまとめた関数を定義\n",
    "# 単語分割の関数を渡すので、tokenizer_bertではなく、tokenizer_bert.tokenizeを渡す点に注意\n",
    "def tokenizer_with_preprocessing(text, tokenizer=tokenizer_bert.tokenize):\n",
    "    text = preprocessing_text(text)\n",
    "    ret = tokenizer(text)  # tokenizer_bert\n",
    "    return ret\n",
    "\n",
    "# データを読み込んだときに、読み込んだ内容に対して行う処理を定義します\n",
    "#max_length=128\n",
    "TEXT = torchtext.data.legacy.Field(sequential=True, tokenize=tokenizer_with_preprocessing, use_vocab=True, lower=False, include_lengths=True, batch_first=True, fix_length=max_length, init_token=\"[CLS]\", eos_token=\"[SEP]\", pad_token=\"[PAD]\",unk_token='[UNK]')\n",
    "\n",
    "LABEL = torchtext.data.legacy.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "# フォルダ「data」から各tsvファイルを読み込みます\n",
    "train_ds, test_ds = torchtext.legacy.data.TabularDataset.splits(\n",
    "    path=DATA_PATH, train='train.tsv',\n",
    "    test='test.tsv', format='tsv',\n",
    "    fields=[('Text', TEXT), ('Label', LABEL)])\n",
    "\n",
    "test_dl = torchtext.legacy.data.Iterator(test_ds, batch_size=batch_size, train=False, sort=False)\n",
    "# 辞書オブジェクトにまとめる\n",
    "dataloaders_dict = {\"test\": test_dl}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ＊検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# テストデータでの正解率を求める\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net_trained.eval()   # モデルを検証モードに\n",
    "net_trained.to(device)  # GPUが使えるならGPUへ送る\n",
    "\n",
    "# epochの正解数を記録する変数\n",
    "epoch_corrects = 0\n",
    "\n",
    "predicted_label=[]#予測ラベル\n",
    "ture_label=[]#正解ラベル\n",
    "\n",
    "score_0=[]#0のスコア\n",
    "score_1=[]#1のスコア\n",
    "count=0\n",
    "\n",
    "start =time.time()\n",
    "for batch in tqdm(test_dl):  # testデータのDataLoader\n",
    "    \n",
    "    # batchはTextとLableの辞書オブジェクト\n",
    "    # GPUが使えるならGPUにデータを送る\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = batch.Text[0].to(device)  # 文章\n",
    "    labels = batch.Label.to(device)  # ラベル\n",
    "    epoch_loss=0.0\n",
    "\n",
    "    \n",
    "    # 順伝搬（forward）計算\n",
    "    with torch.set_grad_enabled(False):\n",
    "\n",
    "        # Bertに入力\n",
    "        outputs = net_trained(inputs, token_type_ids=None, attention_mask=None,\n",
    "                              output_all_encoded_layers=False, attention_show_flg=False)\n",
    "\n",
    "        loss = criterion(outputs, labels)  # 損失を計算\n",
    "        \n",
    "        _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
    "        epoch_corrects += torch.sum(preds == labels.data)  # 正解数の合計を更新\n",
    "        \n",
    "        #outputs=cc(outputs)\n",
    "        for i in range(batch_size):\n",
    "            try:\n",
    "                s1=outputs[i][0].item()\n",
    "                s2=outputs[i][1].item()\n",
    "                p_label = preds[i].item()\n",
    "                t_label = labels[i].item()\n",
    "                score_0.append(s1)\n",
    "                score_1.append(s2)\n",
    "                predicted_label.append(p_label)\n",
    "                ture_label.append(t_label)\n",
    "                count+=1\n",
    "            except:\n",
    "                break\n",
    "        epoch_loss += loss.item() * batch_size\n",
    "        \n",
    "t=time.time()\n",
    "time_test =t-start\n",
    "print(\"Time:{:.4f}sec\".format(time_test))\n",
    "        \n",
    "epoch_loss = epoch_loss / len(test_dl.dataset)\n",
    "print('Loss:{:.4f}'.format(epoch_loss))\n",
    "# 正解率\n",
    "epoch_acc = epoch_corrects.double() / len(test_dl.dataset)\n",
    "\n",
    "print('テストデータ{}個での正解率：{:.4f}'.format(len(test_dl.dataset), epoch_acc))\n",
    "\n",
    "df = pd.read_csv(\"./data/test.csv\", names=(\"TEXT\", \"LABEL\"), engine=\"python\", encoding=\"utf-8-sig\")\n",
    "#df[\"TEXT\"] = np.nan   #予測列を追加\n",
    "#df[\"LABEL\"] = np.nan   #予測列を追加\n",
    "df[\"PREDICT\"] = np.nan   #予測列を追加\n",
    "df[\"AUC+\"] = np.nan   #予測列を追加\n",
    "df[\"AUC-\"] = np.nan   #予測列を追加\n",
    "\n",
    "for index in range(count):\n",
    "    df.at[index, \"PREDICT\"] = predicted_label[index]\n",
    "    \n",
    "    df.at[index, \"AUC+\"] = score_0[index]\n",
    "    df.at[index, \"AUC-\"] = score_1[index]\n",
    "    \n",
    "    \n",
    "df.to_csv(path_result+\"predicted_test.csv\", encoding=\"utf-8-sig\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ＊検証時のROC(AUC)の出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "fpr, tpr, thresholds = roc_curve(ture_label, score_1)\n",
    "plt.axes().set_aspect(\"equal\")\n",
    "#plt.plot(fpr, tpr,marker=\".\")\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.grid()\n",
    "plt.savefig(path_result+\"roc_curve.png\")\n",
    "auc=roc_auc_score(ture_label,score_1)\n",
    "print(\"AUC:{}\".format(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "precision, recall, thresholds = precision_recall_curve(ture_label, score_1)\n",
    "#plt.plot(fpr, tpr,marker=\".\")\n",
    "plt.axes().set_aspect(\"equal\")\n",
    "plt.xlim(0.0, 1.0)\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.plot(recall, precision)\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.grid()\n",
    "plt.savefig(path_result+\"precision_recall.png\")\n",
    "pr_auc=auc(recall, precision)\n",
    "print(\"AUC:{}\".format(pr_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ＊混同行列と精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#混合行列の表示（評価）\n",
    "y_true =[]\n",
    "y_pred =[]\n",
    "df = pd.read_csv(path_result+\"predicted_test.csv\", engine=\"python\", encoding=\"utf-8-sig\")\n",
    "#df = pd.read_csv(\"./result/predicted_test.csv\", engine=\"python\", encoding=\"sjis\")\n",
    "for index, row in df.iterrows():\n",
    "    if row['LABEL'] == 0:\n",
    "        y_true.append(\"負例\")\n",
    "    if row['LABEL'] ==1:\n",
    "        y_true.append(\"正例\")\n",
    "    if row['PREDICT'] ==0:\n",
    "        y_pred.append(\"負例\")\n",
    "    if row['PREDICT'] ==1:\n",
    "        y_pred.append(\"正例\")\n",
    "\n",
    "    \n",
    "print(len(y_true))\n",
    "print(len(y_pred))\n",
    "\n",
    "\n",
    "# 混同行列(confusion matrix)の取得\n",
    "labels = [\"負例\", \"正例\"]\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "# データフレームに変換\n",
    "cm_labeled = pd.DataFrame(cm, columns=labels, index=labels)\n",
    "\n",
    "# 結果の表示\n",
    "cm_labeled.to_csv(path_result+\"confusion_matrix.csv\", encoding=\"utf-8-sig\")\n",
    "cm_labeled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true =[]\n",
    "y_pred =[]\n",
    "df = pd.read_csv(path_result+\"predicted_test.csv\", engine=\"python\", encoding=\"utf-8-sig\")\n",
    "#df = pd.read_csv(\"./result/predicted_test.csv\", engine=\"python\", encoding=\"sjis\")\n",
    "for index, row in df.iterrows():\n",
    "    y_true.append(row[\"LABEL\"])\n",
    "    y_pred.append(row[\"PREDICT\"])\n",
    "\"\"\"       \n",
    "print(\"正解率（すべてのサンプルのうち正解したサンプルの割合）={}%\".format((round(accuracy_score(y_true, y_pred),2)) *100 ))\n",
    "print(\"適合率（positiveと予測された中で実際にpositiveだった確率）={}%\".format((round(precision_score(y_true, y_pred),2)) *100 ))\n",
    "print(\"再現率（positiveなデータに対してpositiveと予測された確率）={}%\".format((round(recall_score(y_true, y_pred),2)) *100 ))\n",
    "print(\"F1（適合率と再現率の調和平均）={}%\".format((round(f1_score(y_true, y_pred),2)) *100 ))\n",
    "\"\"\"\n",
    "print(\"正解率（すべてのサンプルのうち正解したサンプルの割合）={}\".format((accuracy_score(y_true, y_pred))))\n",
    "print(\"適合率（positiveと予測された中で実際にpositiveだった確率）={}\".format((precision_score(y_true, y_pred))))\n",
    "print(\"再現率（positiveなデータに対してpositiveと予測された確率）={}\".format((recall_score(y_true, y_pred))))\n",
    "print(\"F1（適合率と再現率の調和平均）={}\".format((f1_score(y_true, y_pred))))\n",
    "with open(\"{}auc_f.txt\".format(path_result),\"a\",encoding=\"utf-8\") as f:\n",
    "    f.write(\"roc_curve, AUC:{}\\n\".format(auc))\n",
    "    f.write(\"precision_recall, AUC:{}\\n\".format(pr_auc))\n",
    "    f.write(\"正解率（すべてのサンプルのうち正解したサンプルの割合）={}\\n\".format((accuracy_score(y_true, y_pred))))\n",
    "    f.write(\"適合率（positiveと予測された中で実際にpositiveだった確率）={}\\n\".format((precision_score(y_true, y_pred))))\n",
    "    f.write(\"再現率（positiveなデータに対してpositiveと予測された確率）={}\\n\".format((recall_score(y_true, y_pred))))\n",
    "    f.write(\"F1（適合率と再現率の調和平均）={}\\n\".format((f1_score(y_true, y_pred))))\n",
    "    f.write(\"Time_val:{:.4f}sec\\n\".format(time_val))\n",
    "    f.write(\"Time_test:{:.4f}sec\\n\".format(time_test))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ＊Attentionの可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ミニバッチの用意\n",
    "batch = next(iter(test_dl))\n",
    "\n",
    "# GPUが使えるならGPUにデータを送る\n",
    "inputs = batch.Text[0].to(device)  # 文章\n",
    "labels = batch.Label.to(device)  # ラベル\n",
    "\n",
    "outputs, attentions = net_trained(inputs, token_type_ids=None, attention_mask=None,\n",
    "                                       output_all_encoded_layers=False, attention_show_flg=True)\n",
    "\n",
    "_, preds = torch.max(outputs, 1)  # ラベルを予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTMLを作成する関数を実装\n",
    "\n",
    "def highlight(word, attn):\n",
    "    \"Attentionの値が大きいと文字の背景が濃い赤になるhtmlを出力させる関数\"\n",
    "\n",
    "    html_color = '#%02X%02X%02X' % (\n",
    "        255, int(255*(1 - attn)), int(255*(1 - attn)))\n",
    "    return '<span style=\"background-color: {}\"> {}</span>'.format(html_color, word)\n",
    "\n",
    "\n",
    "def mk_html(index, batch, preds, normlized_weights, TEXT):\n",
    "    \"HTMLデータを作成する\"\n",
    "\n",
    "    # indexの結果を抽出\n",
    "    sentence = batch.Text[0][index]  # 文章\n",
    "    label = batch.Label[index]  # ラベル\n",
    "    pred = preds[index]  # 予測\n",
    "\n",
    "    # ラベルと予測結果を文字に置き換え\n",
    "    \"\"\"\n",
    "    if label == 0:\n",
    "        label_str = \"負例\"\n",
    "    else:\n",
    "        label_str = \"正例\"\n",
    "\n",
    "    if pred == 0:\n",
    "        pred_str = \"負例\"\n",
    "    else:\n",
    "        pred_str = \"正例\"\n",
    "    \"\"\"\n",
    "    if label == 0:\n",
    "        label_str = \"非有益\"\n",
    "    else:\n",
    "        label_str = \"有益\"\n",
    "\n",
    "    if pred == 0:\n",
    "        pred_str = \"非有益\"\n",
    "    else:\n",
    "        pred_str = \"有益\"\n",
    "\n",
    "\n",
    "    # 表示用のHTMLを作成する\n",
    "    #html = '正解ラベル：{}<br>推論ラベル：{}<br><br>'.format(label_str, pred_str)\n",
    "    html = '正解ラベル：{}<br>推論ラベル：{}<br>'.format(label_str, pred_str)\n",
    "\n",
    "    # Self-Attentionの重みを可視化。Multi-Headが12個なので、12種類のアテンションが存在\n",
    "    for i in range(12):\n",
    "\n",
    "        # indexのAttentionを抽出と規格化\n",
    "        # 0単語目[CLS]の、i番目のMulti-Head Attentionを取り出す\n",
    "        # indexはミニバッチの何個目のデータかをしめす\n",
    "        attens = normlized_weights[index, i, 0, :]\n",
    "        attens /= attens.max()\n",
    "        \"\"\"\n",
    "        html += '[BERTのAttentionを可視化_' + str(i+1) + ']<br>'\n",
    "        for word, attn in zip(sentence, attens):\n",
    "\n",
    "            # 単語が[SEP]の場合は文章が終わりなのでbreak\n",
    "            if tokenizer_bert.convert_ids_to_tokens([word.numpy().tolist()])[0] == \"[SEP]\":\n",
    "                break\n",
    "\n",
    "            # 関数highlightで色をつける、関数tokenizer_bert.convert_ids_to_tokensでIDを単語に戻す\n",
    "            html += highlight(tokenizer_bert.convert_ids_to_tokens(\n",
    "                [word.numpy().tolist()])[0], attn)\n",
    "        html += \"<br><br>\"\n",
    "        \"\"\"\n",
    "\n",
    "    # 12種類のAttentionの平均を求める。最大値で規格化\n",
    "    all_attens = attens*0  # all_attensという変数を作成する\n",
    "    for i in range(12):\n",
    "        attens += normlized_weights[index, i, 0, :]\n",
    "    attens /= attens.max()\n",
    "\n",
    "    #html += '[BERTのAttentionを可視化_ALL]<br>'\n",
    "    for word, attn in zip(sentence, attens):\n",
    "\n",
    "        # 単語が[SEP]の場合は文章が終わりなのでbreak\n",
    "        \n",
    "        if tokenizer_bert.convert_ids_to_tokens([word.numpy().tolist()])[0] == \"[SEP]\":\n",
    "            break\n",
    "        \n",
    "\n",
    "        # 関数highlightで色をつける、関数tokenizer_bert.convert_ids_to_tokensでIDを単語に戻す\n",
    "        html += highlight(tokenizer_bert.convert_ids_to_tokens(\n",
    "            [word.numpy().tolist()])[0], attn)\n",
    "        \n",
    "    html += \"<br><br>\"\n",
    "\n",
    "    return html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "index = 5  # 出力させたいデータ\n",
    "html_output = mk_html(index, batch, preds, attentions[-1], TEXT)  # HTML作成\n",
    "HTML(html_output)  # HTML形式で出力\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "for batch in test_dl:\n",
    "    # ミニバッチの用意\n",
    "    #batch = next(iter(test_dl))\n",
    "\n",
    "    # GPUが使えるならGPUにデータを送る\n",
    "    inputs = batch.Text[0].to(device)  # 文章\n",
    "    labels = batch.Label.to(device)  # ラベル\n",
    "\n",
    "    outputs, attentions = net_trained(inputs, token_type_ids=None, attention_mask=None,\n",
    "                                           output_all_encoded_layers=False, attention_show_flg=True)\n",
    "    _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
    "    \n",
    "    for index in range(batch_size):\n",
    "        try:\n",
    "            html_output = mk_html(index, batch, preds, attentions[-1], TEXT)  # HTML作成\n",
    "            with open(\"{}Attention.html\".format(path_result),\"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(html_output)\n",
    "            count+=1\n",
    "        except:\n",
    "            pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
